---
title: "Income Regression Analysis: Project report"
output: 
  html_notebook:
    toc: true
    theme: united
---

# Data preparation

## Preprocessing

```{r}
df <- read.csv('dataset.csv')
df$VETSTAT <- as.factor(df$VETSTAT)
df$IND <- as.factor(df$IND)
df$SCHLTYPE_revised <- as.factor(df$SCHLTYPE_revised)
head(df)
```

```{r}
# aggregating the industry variable
par(mfrow=c(1,2))
plot(df$IND)
plot(df$IND_revised)
```
We aggregated the education variable into 4 levels that capture high school, college, and advanced college degrees.
```{r}
# aggregate the education variable
df$EDUC_revised <- as.factor(sapply(df$EDUC, function(x) {
  if (x == 0) return('no school or N/A')
  else if (x < 7) return('1-12 years of pre-college')
  else if (x < 11) return('1-4 years of college')
  else return ('5+ years of college')
}))
par(mfrow=c(1,2))
plot(as.factor(df$EDUC))
plot(df$EDUC_revised)
```

Unsuprisingly, there is an obvious correlation between the education level and income.
```{r}
library(ggplot2)
ggplot(df) +
  geom_point(aes(x = seq(1,nrow(df)), y = df$INCTOT, color=df$EDUC_revised)) +
  theme_bw()
```

```{r}
# aggregate the school type variable
df$SCHLTYPE_revised <- as.factor(sapply(df$SCHLTYPE, function(x) {
  if (x == 0) return('N/A')
  else if (x == 1) return('Not Enrolled')
  else if (x == 2) return('Public School')
  else return ('Private School')
}))
par(mfrow=c(1,2))
plot(as.factor(df$SCHLTYPE))
plot(df$SCHLTYPE_revised)
```

We dropped `INCWAGE` variable after realizing it was not very discriptive. We decided to use `INCTOT` as a response variable.
```{r}
hist(df$INCTOT, breaks = 80)
```

```{r}
# drop old variables
dfn <- subset(df, select = -c(MARRNO_revised, RACE, HCOVANY, IND, VETSTAT, PWSTATE2, EDUC, SCHLTYPE))
# rename columns
colnames(dfn) <- c("WEIGHT", "NUM_CHILD", "SEX", "AGE", "NUM_MARR", "WEEKS_WORKED_NUMERIC", "NUM_HOURS_WEEK", "INCOME", "TRAVEL_TIME", "INDUSTRY", "EDUC", "DIVISION", "SCHL_TYPE", "RACE", "HEALTH_INS", "WEEKS_WORKED", "VETERAN")
head(dfn)
```

In the following script, we calculated the weighted (by `WEIGHT`) income for the observations with equal predictor variables.
```{r}
library(data.table)
dt <- as.data.table(dfn)
# group by predictor variables
dt <- dt[,list(INCOME=weighted.mean(INCOME, WEIGHT)), setdiff(names(dt), c("WEIGHT", "INCOME"))]
head(dt)
```

We still left with around 10,000 rows. 
```{r}

distributions <- function(dt) {
  par(mfrow=c(4,4))
  # show distributions of the variables
  for (i in names(dt)) {
    if (is.numeric(dt[[i]])) {
      hist(dt[[i]], main = i)
    } else {
      plot(dt[[i]], main = i)
    }
  }
}

# visualise distributions
distributions(dt)
```
The goal is to sample the dataset and maintain the distribution properties of the variable
```{r}
set.seed(1337)
dts <- dt[sample(nrow(dt), 500), ]
distributions(dts)
```
As we see, the general distribution properties are captured in the 500 observations subset.

## Documentation
After all preprocessing operations, our dataset contained 16 variables:

| Variable      | Description |
|---------------|-------------------------------------------------------------|
| NUM_CHILD     | Counts the number of own children (of any age or marital status) residing with each individual. Includes step-children and adopted children as well as biological children.
| SEX           | Reports whether the person was male or female.
| AGE           | Reports the person's age in years as of the last birthday.
| EDUC          | Reports the person's education level.
| NUM_MARR      | Reports the number of times ever-married persons have been married.
| NUM_HOURS_WEEK| Reports the usual number of hours the person earned per week.
| TRAVEL_TIME   | Reports the number of minutes it usually took the person to get home from work.
| INDUSTRY      | Reports the type of industry in which the person performed an occupation. Persons who worked in multiple industries were asked to report from which they earned the most money. Unemployed persons were to report their most recent industry. 
| DIVISION      | Reports the region of the country that the person's primary workplace is located. 
| SCHL_TYPE     | Reports whether the person attended public or private school, if they were enrolled at all. 
| RACE          | Reports the race of the person.
| HEALTH_INS    | Reports whether or not the person has health insurance coverage.
| WEEKS_WORKED  | Reports the number of weeks the person worked last year, intervalled. 
| WEEKS_WORKED_NUMERIC| Reports the number of week the person worked last year.
| VETERAN       | Reports whether or not the person is a veteran.
| INCOME        | Reports the total person's pre-tax personal income from the previous year. This is the response variable in the analysis. 

# Multiple Linear Regression Analysis: 

## "Preliminary" model

In this first model, we decided to run multiple regression on income using all 14 of independent variables in the dataset. But before building the model, we made some final preprocessing steps:

(1) `WEEKS_WORKED_NUMERIC` variable has been removed from the data set
(2) `NUM_MARR` variable is converted from numerical variable to categorical variable
(3) long industry names shortened for prettier model summaries

```{r}
orig_data <- read.csv("dataset_final_sample(Connor).csv")
orig_data <- orig_data[, !(colnames(orig_data) %in% c("WEEKS_WORKED_NUMERIC"))]
orig_data$INDUSTRY <- sapply(orig_data$INDUSTRY, function(x) substr(x, 0, 60))
orig_data$NUM_MARR <- as.factor(orig_data$NUM_MARR)
mlr <- lm(orig_data$INCOME ~ ., data = orig_data )
summary(mlr)
```
For this model, $\bar{R}^2$ turnes out to be $0.3092$, which means that the model fails to describe the data accurately. 

### F test and p-value for "preliminary" model

```{r}
qf(0.95, df1 = 38, df2 = 461)
```
Running F-test on "mlr" model reveals that the model is significant at alpha = 0.05 because F-statistic of the model ($6.878$) is greater than $F(0.05) = 1.431182$. Also, p-value is close to being $0$. 

### `VIF` test (multicollinearity test)

```{r}
library(car)
vif(mlr)
mean(vif(mlr)[,1]) # mean of vifs = 1.6133
max(vif(mlr)[,1]) # maximum value of vifs = 2.964278
```
Since mean of vifs = $1.6133$ is not substantially greater than $1$ and maximum value of vifs = $2.964278$ is less than $10$, we can conclude that our "preliminary" model doesn't exhibit multicollinearity.

### Residual analysis of "preliminary" model

```{r}
# Split the plotting panel into a 2 x 2 grid
par(mfrow = c(2, 2), pch = 16, cex = .5)
plot(mlr)
```

Four plots above shows that our model does not satisfy the regression assumptions:

* Specifically, the **Residuals vs. Fitted Values** plot shows an obvious pattern that fans out as the fitted values increase, showing that the variance of the error terms are not constant. 
* The Normal Probability Plot shows curvature toward the ends, indicating that the error terms are not normally distributed. 


## "Manual" model

For the second model, we decided to select variables that passed 95% significance test on the "preliminary" model. They are `AGE`, `NUM_HOURS_WEEK`, `EDUC` and `DIVISION`.

```{r}
mlr_t <- lm(orig_data$INCOME ~ AGE + NUM_HOURS_WEEK + EDUC + DIVISION, data =orig_data)
summary(mlr_t)
```
For this model, $\bar{R}^2$ turned out to be $0.3092$, which is the same as the one we got from "preliminary" model. It means that by reducing the dimensionality, we did not loose much information about the predicted variable.

### F test for "manual" model

```{r}
qf(0.95, df1 = 8, df2 = 491)
```
"Manual" model is statistically significant because F-statistic: $28.9$ is greater than $F(alpha = 0.05) = 1.957253$

### Plot "manual" model

```{r}
par(mfrow = c(2, 2), pch = 16, cex = .5)
plot(mlr_t)
```
Four plots above shows that the second model does not satisfy the regression assumptions. Similarly to the first model, The **Residuals vs. Fitted Values** plot, as well as the **Normal Probability Plot** show variations from the expected plots, indicating the error terms are not normally distributed with constant variance. 

## Automatic model building

### Backward elimination

```{r}
step(mlr, direction = "backward")
```
Running backward elimination method reveals that we should include `SEX`, `AGE`, `NUM_HOURS_WEEK`, `EDUC`, `DIVISION` as our independent variables. It should be noted that these variables are the same as the significant variables chosen for the manual model above, with the addition of the `SEX` variable here. 

### Stepwise regression

```{r}
step(mlr, direction = "both")
```

Running stepwise regression method reveals that we should include `SEX`, `AGE`, `NUM_HOURS_WEEK`, `EDUC`, `DIVISION` as our independent variables. This is the same conclusion drawn from the backward elimination. 

## Multiple Linear Regression: "Automatic" model

Automatic model building procedure suggested that we should use `SEX`, `AGE`, `NUM_HOURS_WEEK`, `EDUC`, `DIVISION` as our independent variables. 
```{r}
mlr_bs <- lm(orig_data$INCOME ~ SEX + AGE + NUM_HOURS_WEEK + EDUC + DIVISION, data = orig_data)
summary(mlr_bs)
```
Adjusted $\bar{R}^2$ turned out to be $0.3108$, which is slightly better than "mlr" model. 

### Residual analysis of "automatic" model

#### Residual plots

```{r}
par(mfrow = c(2, 2), pch = 16, cex = .5)
plot(mlr_bs)
```
Although the $\bar{R}^2$ was slightly improved compared to the prior models, the residual plots still show that our model doesn't satisfy the regression assumptions.  

#### Histogram plot of "automatic" model's residuals.

```{r}
hist(mlr_bs$residuals, col = "lightgray")
```

Plot of residuals confirms that they are definitely not distributed normally, but in fact are positively skewed. We can try to apply some transformations to normalize the residuals.

### Finding Box-Cox transformation's value of lambda

```{r}
library(MASS)
b_bs <- boxcox(mlr, lambda = seq(0,0.2,.1))
boxcox_values_bs <- cbind(b_bs$x,b_bs$y)
value_bs <- boxcox_values_bs[order(-b_bs$y),][1,1]
value_bs # 0.1010101
```
Running Box-Cox test reveals that we should use $\lambda =0.1010101$ for our transformation on our dependent variable which is income. As you can see from above plot, maximum likelihood estimation has the highest peak at $\lambda = 0.1010101$.

## Box-Cox transformed model

We created a new model using transformed predicted variable. Re-running backward elimination again (steps omitted) showed that we should include `AGE`, `NUM_HOURS_WEEK`, `TRAVEL_TIME`,`INDUSTRY`, `EDUC`, `DIVISION`, `SCHL_TYPE`, `HEALTH_INS`, `WEEKS_WORKED`. It is notable that these variables are different than the ones picked out from the non-transformed data: `SEX`, `AGE`, `NUM_HOURS_WEEK`, `EDUC`, `DIVISION`.

```{r}
orig_data$INCOME_TRANSFORMED <- orig_data$INCOME^value_bs
mlr_transform_bs <- lm(formula = orig_data$INCOME_TRANSFORMED ~ AGE + NUM_HOURS_WEEK + 
    TRAVEL_TIME + INDUSTRY + EDUC + DIVISION + SCHL_TYPE + HEALTH_INS + 
    WEEKS_WORKED, data = orig_data)
summary(mlr_transform_bs)
```

The transormed model has $\bar{R}^2$ equal to $0.5848$, which is a big improvement from all previous models. 

## Residual analysis of the transormed model

### Residual plots

```{r}
par(mfrow = c(2, 2), pch = 16, cex = .5)
plot(mlr_transform_bs)
```
The transformed model still seems to not quite satisfy the regression assumptions. There are, however, improvements in the **Normal Probability Plot** and **Residuals vs. Fitted Values** plot, indicating that this model more accurately following the assumptions of constant variance and normality than the previous models discussed:

(1) The Normal Probability Plot appears fairly linear, indicating the normality assumption is satisfied. 
(2) The Residials vs. Fitted Values plot doesn't appear to have any obvious patterns, with only slight fanning out for larger fitted values (suspected outliers). There is no clear indication that the constant variance assumption may be invalidated. 

### Histogram of the transformed model's residuals

```{r}
hist(mlr_transform_bs$residuals, col = "lightgray")
```
Histogram plot above can be said appproximately normal, further indicating that the normality assumption holds for the transformed model. 

*TODO: test Anderson-Darling statistic*.

### Cook's Distance Measure plot for the transformed model

```{r}
library(car)
plot(mlr_transform_bs, pch = 18, col ="red", which = c(4)) 
cook <- round(cooks.distance(mlr_transform_bs), 5)
abline(h = 4*mean(cook, na.rm=T), col="blue")  # add cutoff line
```
```{r}
f_50 <- qf(0.50, df1 = 10, df2 = nrow(orig_data) - 10 )
f_80 <- qf(0.20, df1 = 10, df2 = nrow(orig_data) - 10 )
any(cook > f_50, na.rm = T)
```
There seems to be no influential observations according to cook's distance measure because none of the observations have cook's distance values greater than F[0.5], but every observations have cook's distance values less than F[0.8]

** TODO: I belive that there is something wrong with this calculation. I think Cook's D should have discovered influential observations here. **

### Studentized residuals of the transormed model

```{r}
stud_residual <- rstandard(mlr_transform_bs) #studentized residuals
omit_stud <- which(stud_residual < -2 | stud_residual > 2 | is.na(stud_residual)) #studentized residuals greater than absolute value of 2 and na values
length(omit_stud)
#28 potential outliers wrt y values
```
After studentizing residuals from "mlr_transform_orig_bs" model, we selected 28 potential outliers with respect to $y$ values. 

### Dropping potential outliers from the transormed model

```{r}
orig_data_stud_omit <- orig_data[-omit_stud,]
mlr_transform_orig_bs_omit <- lm(INCOME_TRANSFORMED ~ AGE + NUM_HOURS_WEEK + 
    TRAVEL_TIME + INDUSTRY + EDUC + DIVISION + SCHL_TYPE + HEALTH_INS + 
    WEEKS_WORKED, data = orig_data_stud_omit)
summary(mlr_transform_orig_bs_omit)
```
After dropping 28 observations that were classified as potential outliers model has $\bar{R}^2$ equal to $0.6783$. This is notably increased from the $0.5848$ value before the outliers were dropped. 


```{r}
par(mfrow = c(2, 2), pch = 16, cex = .5)
plot(mlr_transform_orig_bs_omit)
```
4 plots above seem to indicate that the model without suspected outliers satisfies the regresion assumptions. The **Normal Probability Plot** is very linear, and the **Residuals vs. Fitted Value** Plot shows no sighns of violation of the regression assumptions of normalilty and constant variance of the error terms. 

###  Leverage values for the transformed model

```{r}
lev <- hatvalues(mlr_transform_orig_bs_omit)
cutoff_lev <- 2 * 10 / nrow(orig_data)
cutoff_lev
length(lev[lev > cutoff_lev]) #293 observations that are associated with large leverage values
```
There are 293 observations that are associated with large leverage values. We believe that large leverage values are less of a concern. 


# Conclusion

Our preliminary model has adjusted $\bar{R}^2 = 0.3092$ and F-test indicates that the overall model is significant at $\alpha=0.05$. However, plotting residuals reveal that the preliminary model doesn't satisfy the regression assumptions. The manual model doesn't do any better than the preliminary model in terms of adjusted $\bar{R}^2$ because they have same adjusted $\bar{R}^2$ value. Running vif test reveal that the preliminary model doesn't seem to exhibit any serious multicollinearity among dependent variables. We can further narrow down our model by runnning both backward elimination and stepwise regression on preliminary model. Both methods leave us with `SEX`, `AGE`, `NUM_HOURS_WEEK`, `EDUC`, and `DIVISION` variables to run regression on income variable. This automatic model does slightly better than the preliminary model in terms of adjusted $\bar{R}^2$ because now adjusted $\bar{R}^2 = 0.3108$, but plot of residuals shows that "mlr_bs" model still doesn't satisfy the regression assumption. In order to meet the regression assumptions requirement, we had to transform our income, dependent variable, using box-cox transformation mathod. Applying box-cox transformation on the automatic model gives us an automatic transformed model, which has residuals normally distributed and adjusted $\bar{R}^2 = 0.503$. However, plot of residuals doesn't seem to quite satisfy the regression assumption even though distribution of residuals way more closely resembles normal distribution than the preliminary or automatic models. 

It seems that we can do better by applying boxcox transformation on the preliminary model first before applying stepwise regression or backward elimination method. If we box-cox transform our income variable first, we get a preliminary transformed model, which has adjusted $\bar{R}^2 = 0.5836$. Then, we sequentially apply stepwise regression or backward elimination method on this model. Both methods give us two identical models using same independent variables. Finally, we can test for influential observations wrt both y and x values. Cook's distance measure test indicate that we don't seem to have any influential observations wrt both x and y values. However, applying studentized residuals statistical test on "mlr_transform_orig" model reveal that we have 28 potential outliers that need to be taken care of. We decided to exclude those 28 outliers and ran a new regression using reduced data set. At last, we have achieved our best model so far. The preliminary transformed model has adjusted $\bar{R}^2 = 0.6783$ after stepwise regression, and plots of residuals approximately satisfies the regression assumption even though we have few observations that are possibly outliers wrt to either y or x values.  
