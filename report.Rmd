---
title: "Stats project report"
output: html_notebook
---

# Data preparation and visualization
**bold text** 

```{r}
df <- read.csv('dataset.csv')
df$VETSTAT <- as.factor(df$VETSTAT)
df$IND <- as.factor(df$IND)
df$SCHLTYPE_revised <- as.factor(df$SCHLTYPE_revised)
head(df)
```

```{r}
# aggregating the industry variable
par(mfrow=c(1,2))
plot(df$IND)
plot(df$IND_revised)
```
We aggregated the education variable into 4 levels that capture high school, college, and advanced college degrees.
```{r}
# aggregate the education variable
df$EDUC_revised <- as.factor(sapply(df$EDUC, function(x) {
  if (x == 0) return('no school or N/A')
  else if (x < 7) return('1-12 years of pre-college')
  else if (x < 11) return('1-4 years of college')
  else return ('5+ years of college')
}))
par(mfrow=c(1,2))
plot(as.factor(df$EDUC))
plot(df$EDUC_revised)
```

Unsuprisingly, there is an obvious correlation between the education level and income.
```{r}
library(ggplot2)
ggplot(df) +
  geom_point(aes(x = seq(1,nrow(df)), y = df$INCTOT, color=df$EDUC_revised)) +
  theme_bw()
```

```{r}
# aggregate the school type variable
df$SCHLTYPE_revised <- as.factor(sapply(df$SCHLTYPE, function(x) {
  if (x == 0) return('N/A')
  else if (x == 1) return('Not Enrolled')
  else if (x == 2) return('Public School')
  else return ('Private School')
}))
par(mfrow=c(1,2))
plot(as.factor(df$SCHLTYPE))
plot(df$SCHLTYPE_revised)
```

We dropped `INCWAGE` variable after realizing it was not very discriptive. We decided to use `INCTOT` as a response variable.
```{r}
hist(df$INCTOT, breaks = 80)
```
```{r}
# save data
nrow(df)
write.csv(df, file='dataset.csv', col.names=T, row.names = F)
```

```{r}
# drop old variables
dfn <- subset(df, select = -c(MARRNO_revised, RACE, HCOVANY, IND, VETSTAT, PWSTATE2, EDUC, SCHLTYPE))
# rename columns
colnames(dfn) <- c("WEIGHT", "NUM_CHILD", "SEX", "AGE", "NUM_MARR", "WEEKS_WORKED_NUMERIC", "NUM_HOURS_WEEK", "INCOME", "TRAVEL_TIME", "INDUSTRY", "EDUC", "DIVISION", "SCHL_TYPE", "RACE", "HEALTH_INS", "WEEKS_WORKED", "VETERAN")
head(dfn)
```

In the following script, we calculated the weighted (by `WEIGHT`) income for the observations with equal predictor variables.
```{r}
library(data.table)
dt <- as.data.table(dfn)
# group by predictor variables
dt <- dt[,list(INCOME=weighted.mean(INCOME, WEIGHT)), setdiff(names(dt), c("WEIGHT", "INCOME"))]
head(dt)
```
```{r}
# save new dataset
write.csv(dt, file='dataset_final.csv', col.names=T, row.names = F)
```

We still left with around 10,000 rows. 
```{r}

distributions <- function(dt) {
  par(mfrow=c(4,4))
  # show distributions of the variables
  for (i in names(dt)) {
    if (is.numeric(dt[[i]])) {
      hist(dt[[i]], main = i)
    } else {
      plot(dt[[i]], main = i)
    }
  }
}

# visualise distributions
distributions(dt)
```
The goal is to sample the dataset and maintain the distribution properties of the variable
```{r}
set.seed(1337)
dts <- dt[sample(nrow(dt), 500), ]
distributions(dts)
```
As we see, the general distribution properties are captured in the 500 observations subset.

# Multiple Linear Regression Analysis: "mlr" model

```{r}
orig_data <- read.csv("dataset_final_sample(Connor).csv")
orig_data <- orig_data[, !(colnames(orig_data) %in% c("WEEKS_WORKED_NUMERIC"))]
orig_data$NUM_MARR <- as.factor(orig_data$NUM_MARR)
mlr <- lm(orig_data$INCOME ~ ., data = orig_data )
summary(mlr)
```
Comments: 
(1) WEEKS_WORKED_NUMERIC variable has been removed from the data set 
(2) NUM_MARR variable is converted from numerical variable to categorical variable
(3) Ran multiple regression on income using all the independent variables in the dataset
(4) Adjusted R^2 turned out to be 0.3092


# F test and p-value for "mlr" model

```{r}
qf(0.95, df1 = 38, df2 = 461)
```
Comments: 
(1) Running F-test on "mlr" model reveals that the model is significantn at alpha = 0.05 because F-statistic of the model(6.878) is greater than F(0.05) = 1.431182. Also, p-value is close to being 0. 

# plot "mlr"

```{r}
plot(mlr)
```
Comments:
(1) Four plots above shows that our model does not satisfy the regression assumptions

# Multiple Linear Regression: "mlr_t" model

```{r}
mlr_t <- lm(orig_data$INCOME ~ AGE + NUM_HOURS_WEEK + EDUC + DIVISION, data =orig_data)
summary(mlr_t)
```
Comments:
(1) Ran multiple regression on income using variables that were statistically signifcant in the "mlr" model. Those independent variables include AGE, NUM_HOURS_WEEK, EDUC, DIVISION
(2) Adjusted R^2 turned out to be 0.3092, which is the same as the one we got from "mlr" model.  

# F test for "mlr_t" model

```{r}
qf(0.95, df1 = 8, df2 = 491)
```
Comments:
(1) "mlr_t" model is statistically significant because F-statistic: 28.9 is greater than F(alpha = 0.05) = 1.957253

# Plot "mlr_t"

```{r}
plot(mlr_t)
```
Comments:
(1) Four plots above shows that our model does not satisfy the regression assumptions

# vif test (multicollinearity test)

```{r}
library(car)
vif(mlr)
mean(vif(mlr)[,1]) # mean of vifs = 1.6133
max(vif(mlr)[,1]) # maximum value of vifs = 2.964278
```
Comments:
(1) Since mean of vifs = 1.6133 is not substantially greater than 1 and maximum value of vifs = 2.964278 is less than 10, we can conclude that our "mlr" model doesn't exhibit multicollinearity.

# backward elimination

```{r}
step(mlr, direction = "backward")
```
Comments:
(1) Running backward elimination method reveals that we should include SEX, AGE, NUM_HOURS_WEEK, EDUC, DIVISION as our independent variables. 


# stepwise regression

```{r}
step(mlr, direction = "both")
```
Comments:
(1) Running stepwise regression method reveals that we should include SEX, AGE, NUM_HOURS_WEEK, EDUC, DIVISION as our independent variables. 

# Multiple Linear Regression: "mlr_bs" model

```{r}
mlr_bs <- lm(orig_data$INCOME ~ SEX + AGE + NUM_HOURS_WEEK + EDUC + DIVISION, data = orig_data)
summary(mlr_bs)
```
Comments:
(1) Ran regression on income using variables that conform to both backward elimination and stepwise regression results discussed above. Those independent variables include SEX, AGE, NUM_HOURS_WEEK, EDUC, DIVISION. 
(2) Adjusted R^2 turned out to be 0.3108, which is slightly better than "mlr" model. 

# plot of "mlr_bs" model

```{r}
plot(mlr_bs)
```
Comments:
(1) Four plots show that our "mlr_bs" model don't satisfy the regression assumptions.         
# histogram plot of "mlr_bs" model's residuals.

```{r}
hist(mlr$residuals)
```
Comments:
(1) Plot of residuals reveal that they are definitely not distributed normally. 

# Finding boxcox transformation's value of lambda for "mlr_bs" model

```{r}
library(MASS)
b_bs <- boxcox(mlr_bs, lambda = seq(0,0.2,.1))
b_bs
boxcox_values_bs <- cbind(b_bs$x,b_bs$y)
boxcox_values_bs[order(-b_bs$y),]
value_bs <- boxcox_values_bs[order(-b_bs$y),][1,1]
value_bs # 0.1030303
```
Comments:
(1) Running box-cox test reveals that we should use lambda = 0.1030303 for our transformation on our dependent variable which is income. As you can see from above plot, maximum likelihood estimation has the highest peak at lambda = 0.1030303

# boxcox transformation on "mlr_bs" model

```{r}
orig_data <- cbind(orig_data, orig_data$INCOME^value_bs)
names(orig_data)[16] <- "Income_transform_bs"
mlr_transform_bs <- lm(orig_data$Income_transform_bs ~ SEX + AGE + NUM_HOURS_WEEK + EDUC + DIVISION, data = orig_data)
summary(mlr_transform_bs)
```
Comments:
(1) Ran regression on box-cox transformed income using SEX, AGE, NUM_HOURS_WEEK, EDUC, DIVISION variables. 
(2) Our new "mlr_transform_bs" has adjusted R^2 equal to 0.503.

# general plot of "mlr_transform_bs" model

```{r}
plot(mlr_transform_bs)
```
Comments:
(1) "mlr_transform_bs" model seem to not quite satisfy the regression assumptions.

# Plot of "mlr_transform_bs" model's residuals

```{r}
hist(mlr_transform_bs$residuals)
```
Comments:
(1) Histogram plot above can be said appproximately normal. 

# boxcox transformation on "mlr" model

```{r}
library(MASS)
b_orig <- boxcox(mlr, lambda = seq(0,0.2,.1))
boxcox_values_orig <- cbind(b_orig$x,b_orig$y)
boxcox_values_orig[order(-b_orig$y),] 
value_orig <- boxcox_values_orig[order(-b_orig$y),][1,1]
value_orig #lambda = 0.010101010
orig_data <- cbind(orig_data, orig_data$INCOME^value_orig)
names(orig_data)[17] <- "Income_transform_orig"
mlr_transform_orig <- lm(orig_data$Income_transform_orig ~. -INCOME -Income_transform_bs, data = orig_data)
summary(mlr_transform_orig)
```
Comments:
(1) Ran regression on box-cox transformed income using all the independent variables.
(2) "mlr_transform_orig" model's adjusted R^2 is equal to 0.5836

# plot of "mlr_transform_orig" model

```{r}
plot(mlr_transform_orig)
```
Comments:
(1) Looking at four plots reveal that "mlr_transform_orig" model doesn't quite satisfy the regression assumptions. 

# histogram plot of "mlr_transform_orig" model's residuals

```{r}
hist(mlr_transform_orig$residuals)
```
Comments:
(1) We can see that plot of residuals are close to being normally distributed. 

# backward elimination applied on "mlr_transform_orig" model

```{r}
step(mlr_transform_orig, direction = "backward")
```
Comments:
(1) Applying backward elimination on "mlr_transform_orig" model reveals that using AGE, NUM_HOURS_WEEK, TRAVEL_TIME, INDUSTRY, EDUC, DIVISION, SCHL_TYPE, HEALTH_INS, and WEEKS_WORKED yields the best model.


#stepwise regression applied on "mlr_transform_orig" model

```{r}
step(mlr_transform_orig, direction = "both")
```
Comments:
(1) Applying backward elimination on "mlr_transform_orig" model reveals that using AGE, NUM_HOURS_WEEK, TRAVEL_TIME, INDUSTRY, EDUC, DIVISION, SCHL_TYPE, HEALTH_INS, and WEEKS_WORKED yields the best model.

# Multiple linear regression: "mlr_transform_orig_bs" model  

```{r}
mlr_transform_orig_bs <- lm(orig_data$Income_transform_orig ~ AGE + NUM_HOURS_WEEK + 
    TRAVEL_TIME + INDUSTRY + EDUC + DIVISION + SCHL_TYPE + HEALTH_INS + 
    WEEKS_WORKED, data = orig_data)
summary(mlr_transform_orig_bs)
```
Comments:
(1) Ran regression on box-cox transformed income using variables selected by both stepwise regression and backward elimination applied on "mlr_transform_orig" model.
(2) "mlr_transform_orig" model has adjusted R^2 equal to 0.5848

#Cook's Distance Measure plot of "mlr_transform_orig_bs" model

```{r}
library(car)
plot(mlr_transform_orig_bs, pch = 18, col ="red", which = c(4)) 
cook <- round(cooks.distance(mlr_transform_orig_bs), 5)
sort(cook) #sorted in ascending order of cook values
f_50 <- qf(0.50, df1 = 10, df2 = nrow(orig_data) - 10 )
f_50 #calcuation of F[0.5]
cook[cook > f_50]
f_80 <- qf(0.20, df1 = 10, df2 = nrow(orig_data) - 10 )
f_80 #calcuation of F[0.8]
length(cook[cook < f_80])
```
Comments:
(1) There seems to be no influential observations according to cook's distance measure because none of the observations have cook's distance values greater than F[0.5], but every observations have cook's distance values less than F[0.8]

# Studentized residuals of "mlr_transform_orig_bs" model

```{r}
stud_residual <- rstandard(mlr_transform_orig_bs) #studentized residuals
omit_stud <-  stud_residual[stud_residual < -2 | stud_residual > 2 | is.na(stud_residual)] #studentized residuals greater than absolute value of 2 and na values
omit_stud
length(stud_residual[stud_residual < -2 | stud_residual > 2 | is.na(stud_residual)])
#28 potential outliers wrt y values
```
Comments:
(1) After studentizing residuals from "mlr_transform_orig_bs" model, we selected 28 potential outliers wrt y values. 

# Dropping 28 potential outliers from "mlr_transform_orig_bs" model and running new regression

```{r}
orig_data_temp <- orig_data
orig_data_temp <- cbind(orig_data_temp, stud_residual)
orig_data_stud_omit <- orig_data[!(orig_data_temp$stud_residual %in% omit_stud),]
mlr_transform_orig_bs_omit <- lm(orig_data_stud_omit$Income_transform_orig ~ AGE + NUM_HOURS_WEEK + TRAVEL_TIME + INDUSTRY + EDUC + DIVISION + SCHL_TYPE + HEALTH_INS + 
WEEKS_WORKED, data = orig_data_stud_omit)
summary(mlr_transform_orig_bs_omit)
```
Comments:
(1) Ran regression on box-cox transformed income using AGE, NUM_HOURS_WEEK, TRAVEL_TIME, INDUSTRY, EDUC, DIVISION, SCHL_TYPE, HEALTH_INS, and WEEKS_WORKED variables after dropping 28 observations that were classified as potential outliers.


```{r}
plot(mlr_transform_orig_bs_omit)
```
Comments:
(1) 4 plots above seem to indicate that "mlr_transform_orig_bs_omit" model satisfy regresion assumptions. 

# Leverage values for "mlr_transform_orig_bs" model

```{r}
lev <- hatvalues(mlr_transform_orig_bs)
cutoff_lev <- 2 * 10 / nrow(orig_data)
cutoff_lev
lev[lev > cutoff_lev]
length(lev[lev > cutoff_lev]) #293 observations that are associated with large leverage values
```
Comments: There are 293 observations that are associated with large leverage values. We believe that large leverage values are less of a concern. 


```{r}

```

```{r}

```

```{r}

```