---
title: "Stats project report"
output: html_notebook
---

# Data preparation and visualization
**bold text** 

```{r}
df <- read.csv('dataset.csv')
df$VETSTAT <- as.factor(df$VETSTAT)
df$IND <- as.factor(df$IND)
df$SCHLTYPE_revised <- as.factor(df$SCHLTYPE_revised)
head(df)
```

```{r}
# aggregating the industry variable
par(mfrow=c(1,2))
plot(df$IND)
plot(df$IND_revised)
```
We aggregated the education variable into 4 levels that capture high school, college, and advanced college degrees.
```{r}
# aggregate the education variable
df$EDUC_revised <- as.factor(sapply(df$EDUC, function(x) {
  if (x == 0) return('no school or N/A')
  else if (x < 7) return('1-12 years of pre-college')
  else if (x < 11) return('1-4 years of college')
  else return ('5+ years of college')
}))
par(mfrow=c(1,2))
plot(as.factor(df$EDUC))
plot(df$EDUC_revised)
```

Unsuprisingly, there is an obvious correlation between the education level and income.
```{r}
library(ggplot2)
ggplot(df) +
  geom_point(aes(x = seq(1,nrow(df)), y = df$INCTOT, color=df$EDUC_revised)) +
  theme_bw()
```

```{r}
# aggregate the school type variable
df$SCHLTYPE_revised <- as.factor(sapply(df$SCHLTYPE, function(x) {
  if (x == 0) return('N/A')
  else if (x == 1) return('Not Enrolled')
  else if (x == 2) return('Public School')
  else return ('Private School')
}))
par(mfrow=c(1,2))
plot(as.factor(df$SCHLTYPE))
plot(df$SCHLTYPE_revised)
```

We dropped `INCWAGE` variable after realizing it was not very discriptive. We decided to use `INCTOT` as a response variable.
```{r}
hist(df$INCTOT, breaks = 80)
```
```{r}
# save data
nrow(df)
write.csv(df, file='dataset.csv', col.names=T, row.names = F)
```

```{r}
# drop old variables
dfn <- subset(df, select = -c(MARRNO_revised, RACE, HCOVANY, IND, VETSTAT, PWSTATE2, EDUC, SCHLTYPE))
# rename columns
colnames(dfn) <- c("WEIGHT", "NUM_CHILD", "SEX", "AGE", "NUM_MARR", "WEEKS_WORKED_NUMERIC", "NUM_HOURS_WEEK", "INCOME", "TRAVEL_TIME", "INDUSTRY", "EDUC", "DIVISION", "SCHL_TYPE", "RACE", "HEALTH_INS", "WEEKS_WORKED", "VETERAN")
head(dfn)
```

In the following script, we calculated the weighted (by `WEIGHT`) income for the observations with equal predictor variables.
```{r}
library(data.table)
dt <- as.data.table(dfn)
# group by predictor variables
dt <- dt[,list(INCOME=weighted.mean(INCOME, WEIGHT)), setdiff(names(dt), c("WEIGHT", "INCOME"))]
head(dt)
```
```{r}
# save new dataset
write.csv(dt, file='dataset_final.csv', col.names=T, row.names = F)
```

We still left with around 10,000 rows. 
```{r}

distributions <- function(dt) {
  par(mfrow=c(4,4))
  # show distributions of the variables
  for (i in names(dt)) {
    if (is.numeric(dt[[i]])) {
      hist(dt[[i]], main = i)
    } else {
      plot(dt[[i]], main = i)
    }
  }
}

# visualise distributions
distributions(dt)
```
The goal is to sample the dataset and maintain the distribution properties of the variable
```{r}
set.seed(1337)
dts <- dt[sample(nrow(dt), 500), ]
distributions(dts)
```
As we see, the general distribution properties are captured in the 500 observations subset.

# Multiple Linear Regression Analysis: "mlr" model

```{r}
orig_data <- read.csv("dataset_final_sample(Connor).csv")
orig_data <- orig_data[, !(colnames(orig_data) %in% c("WEEKS_WORKED_NUMERIC"))]
orig_data$NUM_MARR <- as.factor(orig_data$NUM_MARR)
mlr <- lm(orig_data$INCOME ~ ., data = orig_data )
summary(mlr)
```
Comments: 
(1) WEEKS_WORKED_NUMERIC variable has been removed from the data set 
(2) NUM_MARR variable is converted from numerical variable to categorical variable
(3) Ran multiple regression on income using all the independent variables in the dataset
(4) Adjusted R^2 turned out to be 0.3092


# F test and p-value for "mlr" model

```{r}
qf(0.95, df1 = 38, df2 = 461)
```
Comments: 
(1) Running F-test on "mlr" model reveals that the model is significant at alpha = 0.05 because F-statistic of the model(6.878) is greater than F(0.05) = 1.431182. Also, p-value is close to being 0. 

# plot "mlr"

```{r}
plot(mlr)
```
Comments:
(1) Four plots above shows that our model does not satisfy the regression assumptions
(2) Specifically, the Residuals vs. Fitted Values plot shows an obvious pattern that fans out as the fitted values increase, showing that the variance of the error terms are not constant. 
(3) The Normal Probability Plot shows curvature toward the ends, indicating that the error terms are not normally distributed. 

# Multiple Linear Regression: "mlr_t" model

```{r}
mlr_t <- lm(orig_data$INCOME ~ AGE + NUM_HOURS_WEEK + EDUC + DIVISION, data =orig_data)
summary(mlr_t)
```
Comments:
(1) Ran multiple regression on income using variables that were statistically signifcant in the "mlr" model. Those independent variables include AGE, NUM_HOURS_WEEK, EDUC, DIVISION
(2) Adjusted R^2 turned out to be 0.3092, which is the same as the one we got from "mlr" model.  

# F test for "mlr_t" model

```{r}
qf(0.95, df1 = 8, df2 = 491)
```
Comments:
(1) "mlr_t" model is statistically significant because F-statistic: 28.9 is greater than F(alpha = 0.05) = 1.957253

# Plot "mlr_t"

```{r}
plot(mlr_t)
```
Comments:
(1) Four plots above shows that our model does not satisfy the regression assumptions
(2) Similarly to the mlr model above, The Residuals vs. Fitted Values plot, as well as the Normal Probability Plot show variations from the expected plots, indicating the error terms are not normally distributed with constant variance. 

# vif test (multicollinearity test)

```{r}
library(car)
vif(mlr)
mean(vif(mlr)[,1]) # mean of vifs = 1.6133
max(vif(mlr)[,1]) # maximum value of vifs = 2.964278
```
Comments:
(1) Since mean of vifs = 1.6133 is not substantially greater than 1 and maximum value of vifs = 2.964278 is less than 10, we can conclude that our "mlr" model doesn't exhibit multicollinearity.

# backward elimination

```{r}
step(mlr, direction = "backward")
```
Comments:
(1) Running backward elimination method reveals that we should include SEX, AGE, NUM_HOURS_WEEK, EDUC, DIVISION as our independent variables. 
(2) It should be noted that these variables are the same as the significant variables chosen for the mlr_t test above, with the addition of the SEX variable here. 


# stepwise regression

```{r}
step(mlr, direction = "both")
```
Comments:
(1) Running stepwise regression method reveals that we should include SEX, AGE, NUM_HOURS_WEEK, EDUC, DIVISION as our independent variables. 
(2) This is the same conclusion drawn from the backward elimination. 

# Multiple Linear Regression: "mlr_bs" model

```{r}
mlr_bs <- lm(orig_data$INCOME ~ SEX + AGE + NUM_HOURS_WEEK + EDUC + DIVISION, data = orig_data)
summary(mlr_bs)
```
Comments:
(1) Ran regression on income using variables that conform to both backward elimination and stepwise regression results discussed above. Those independent variables include SEX, AGE, NUM_HOURS_WEEK, EDUC, DIVISION. 
(2) Adjusted R^2 turned out to be 0.3108, which is slightly better than "mlr" model. 

# plot of "mlr_bs" model

```{r}
plot(mlr_bs)
```
Comments:
(1) Although the adjusted R^2 was slightly improved from the mlr and mlr_t models, the four plots still show that our "mlr_bs" model doesn't satisfy the regression assumptions.  

# histogram plot of "mlr_bs" model's residuals.

```{r}
hist(mlr_bs$residuals)
```
Comments:
(1) Plot of residuals confirms that they are definitely not distributed normally, but in fact are positively skewed. 

# Finding boxcox transformation's value of lambda for "mlr_bs" model

```{r}
library(MASS)
b_bs <- boxcox(mlr_bs, lambda = seq(0,0.2,.1))
b_bs
boxcox_values_bs <- cbind(b_bs$x,b_bs$y)
boxcox_values_bs[order(-b_bs$y),]
value_bs <- boxcox_values_bs[order(-b_bs$y),][1,1]
value_bs # 0.1030303
```
Comments:
(1) Running box-cox test reveals that we should use lambda = 0.1030303 for our transformation on our dependent variable which is income. As you can see from above plot, maximum likelihood estimation has the highest peak at lambda = 0.1030303

# boxcox transformation on "mlr_bs" model

```{r}
orig_data <- cbind(orig_data, orig_data$INCOME^value_bs)
names(orig_data)[16] <- "Income_transform_bs"
mlr_transform_bs <- lm(orig_data$Income_transform_bs ~ SEX + AGE + NUM_HOURS_WEEK + EDUC + DIVISION, data = orig_data)
summary(mlr_transform_bs)
```
Comments:
(1) Ran regression on box-cox transformed income using SEX, AGE, NUM_HOURS_WEEK, EDUC, DIVISION variables. 
(2) Our new "mlr_transform_bs" has adjusted R^2 equal to 0.503. This is improved from all previous models. 

# general plot of "mlr_transform_bs" model

```{r}
plot(mlr_transform_bs)
```
Comments:
(1) "mlr_transform_bs" model seem to not quite satisfy the regression assumptions.
(2) There are, however, improvements in the Normal Probability Plot and Residuals vs. Fitted Values plot, indicating that this model more accurately following the assumptions of constant variance and normality than the previous models discussed. 

# Plot of "mlr_transform_bs" model's residuals

```{r}
hist(mlr_transform_bs$residuals)
```
Comments:
(1) Histogram plot above can be said appproximately normal, further indicating that the normality assumption holds for the transformed model. 

# boxcox transformation on "mlr" model
```{r}
library(MASS)
b_orig <- boxcox(mlr, lambda = seq(0,0.2,.1))
boxcox_values_orig <- cbind(b_orig$x,b_orig$y)
boxcox_values_orig[order(-b_orig$y),] 
value_orig <- boxcox_values_orig[order(-b_orig$y),][1,1]
value_orig #lambda = 0.010101010
orig_data <- cbind(orig_data, orig_data$INCOME^value_orig)
names(orig_data)[17] <- "Income_transform_orig"
mlr_transform_orig <- lm(orig_data$Income_transform_orig ~. -INCOME -Income_transform_bs, data = orig_data)
summary(mlr_transform_orig)
```
Comments:
(1) Ran regression on box-cox transformed income using all the independent variables.
(2) "mlr_transform_orig" model's adjusted R^2 is equal to 0.5836.
(3) This adjusted R^2 is improved from the previous model. 

# plot of "mlr_transform_orig" model

```{r}
plot(mlr_transform_orig)
```
Comments:
(1) Looking at four plots reveal that "mlr_transform_orig" model doesn't quite satisfy the regression assumptions. 
(2) The Normal Probability Plot appears fairly linear, indicating the normality assumption is satisfied. 
(3) The Residials vs. Fitted Values plot doesn't appear to have any obvious patterns, with no clear indication that the constant variance assumption may be invalidated. 

# histogram plot of "mlr_transform_orig" model's residuals

```{r}
hist(mlr_transform_orig$residuals)
```
Comments:
(1) We can see that plot of residuals are close to being normally distributed. 

# backward elimination applied on "mlr_transform_orig" model

```{r}
step(mlr_transform_orig, direction = "backward")
```
Comments:
(1) Applying backward elimination on "mlr_transform_orig" model reveals that using AGE, NUM_HOURS_WEEK, TRAVEL_TIME, INDUSTRY, EDUC, DIVISION, SCHL_TYPE, HEALTH_INS, and WEEKS_WORKED yields the best model.


# stepwise regression applied on "mlr_transform_orig" model

```{r}
step(mlr_transform_orig, direction = "both")
```
Comments:
(1) Applying backward elimination on "mlr_transform_orig" model reveals that using AGE, NUM_HOURS_WEEK, TRAVEL_TIME, INDUSTRY, EDUC, DIVISION, SCHL_TYPE, HEALTH_INS, and WEEKS_WORKED yields the best model.
(2) Again, the stepwise regression picked out the same variables as the backward elimination model. It is notable that these variables are different than the ones picked out from the non-transformed data: SEX, AGE, NUM_HOURS_WEEK, EDUC, DIVISION.

# Multiple linear regression: "mlr_transform_orig_bs" model  

```{r}
mlr_transform_orig_bs <- lm(orig_data$Income_transform_orig ~ AGE + NUM_HOURS_WEEK + 
    TRAVEL_TIME + INDUSTRY + EDUC + DIVISION + SCHL_TYPE + HEALTH_INS + 
    WEEKS_WORKED, data = orig_data)
summary(mlr_transform_orig_bs)
```
Comments:
(1) Ran regression on box-cox transformed income using variables selected by both stepwise regression and backward elimination applied on "mlr_transform_orig" model.
(2) "mlr_transform_orig_bs" model has adjusted R^2 equal to 0.5848. This is approximately equal to the adjusted R^2 value from the "mlr_transform_orig" model. 

# Cook's Distance Measure plot of "mlr_transform_orig_bs" model

```{r}
library(car)
plot(mlr_transform_orig_bs, pch = 18, col ="red", which = c(4)) 
cook <- round(cooks.distance(mlr_transform_orig_bs), 5)
sort(cook) #sorted in ascending order of cook values
f_50 <- qf(0.50, df1 = 10, df2 = nrow(orig_data) - 10 )
f_50 #calcuation of F[0.5]
cook[cook > f_50]
f_80 <- qf(0.20, df1 = 10, df2 = nrow(orig_data) - 10 )
f_80 #calcuation of F[0.8]
length(cook[cook < f_80])
```
Comments:
(1) There seems to be no influential observations according to cook's distance measure because none of the observations have cook's distance values greater than F[0.5], but every observations have cook's distance values less than F[0.8]

# Studentized residuals of "mlr_transform_orig_bs" model

```{r}
stud_residual <- rstandard(mlr_transform_orig_bs) #studentized residuals
omit_stud <-  stud_residual[stud_residual < -2 | stud_residual > 2 | is.na(stud_residual)] #studentized residuals greater than absolute value of 2 and na values
omit_stud
length(stud_residual[stud_residual < -2 | stud_residual > 2 | is.na(stud_residual)])
#28 potential outliers wrt y values
```
Comments:
(1) After studentizing residuals from "mlr_transform_orig_bs" model, we selected 28 potential outliers wrt y values. 

# Dropping 28 potential outliers from "mlr_transform_orig_bs" model and running new regression

```{r}
orig_data_temp <- orig_data
orig_data_temp <- cbind(orig_data_temp, stud_residual)
orig_data_stud_omit <- orig_data[!(orig_data_temp$stud_residual %in% omit_stud),]
mlr_transform_orig_bs_omit <- lm(orig_data_stud_omit$Income_transform_orig ~ AGE + NUM_HOURS_WEEK + TRAVEL_TIME + INDUSTRY + EDUC + DIVISION + SCHL_TYPE + HEALTH_INS + 
WEEKS_WORKED, data = orig_data_stud_omit)
summary(mlr_transform_orig_bs_omit)
```
Comments:
(1) Ran regression on box-cox transformed income using AGE, NUM_HOURS_WEEK, TRAVEL_TIME, INDUSTRY, EDUC, DIVISION, SCHL_TYPE, HEALTH_INS, and WEEKS_WORKED variables after dropping 28 observations that were classified as potential outliers.
(2) "mlr_transform_orig_bs" model has adjusted R^2 equal to 0.6783. This is notably increased from the 0.5848 value before the outliers were dropped. 


```{r}
plot(mlr_transform_orig_bs_omit)
```
Comments:
(1) 4 plots above seem to indicate that "mlr_transform_orig_bs_omit" model satisfy regresion assumptions. 
(2) The Normal Probability Plot is very linear, and the Residuals vs. Fitted Values Plot shows no patterns, indicating validation for the regression assumptions of normalilty and constant variance of the error terms. 

# Leverage values for "mlr_transform_orig_bs" model

```{r}
lev <- hatvalues(mlr_transform_orig_bs)
cutoff_lev <- 2 * 10 / nrow(orig_data)
cutoff_lev
lev[lev > cutoff_lev]
length(lev[lev > cutoff_lev]) #293 observations that are associated with large leverage values
```
Comments: 
(1) There are 293 observations that are associated with large leverage values. We believe that large leverage values are less of a concern. 


# Conclusion:
Our "mlr" model has adjusted R^2 = 0.3092 and F-test indicates that the overall model is significant at alpha=0.05. However, plotting residuls reveal that "mlr" model doesn't satisfy the regression assumptions. "mlr_t" model doesn't do any better than "mlr" model in terms of adjusted R^2 because they have same adjusted R^2 value. Running vif test reveal that "mlr" model doesn't seem to exhibit any serious multicollinearity among dependent variables. We can further narrow down our model by runnning both backward elimination and stepwise regression on "mlr" model. Both methods leave us with SEX, AGE, NUM_HOURS_WEEK, EDUC, and DIVISION variables to run regression on income variable. "mlr_bs" model does slightly better than "mlr" model in terms of adjusted R^2 because now adjusted R^2 is 0.3108, but plot of residuals shows that "mlr_bs" model still doesn't satisfy the regression assumption. In order to meet the regression assumptions requirement, we had to transform our income, depedent variable, using box-cox transformation mathod. Applying box-cox transformation on "mlr_bs" model gives us "mlr_transform_bs" model, which has residuals normally distributed and adjusted R^2 equal to 0.503. However, plot of residuals doesn't seem to quite satisfy the regression assumption even though distribution of residuals way more closely resembles noraml distribution than "mlr" or "mlr_bs" model. 

It seems that we can do better by applying boxcox transformation on "mlr" model(the original model) first before applying stepwise regression or backward elimination method. If we box-cox transform our income variable first, we get "mlr_transform_orig" model, which has adjusted R^2 is equal to 0.5836. Then, we sequentially apply stepwise regression or backward elimination method on "mlr_transform_orig" model. Both methods give us two identical models using same independent variables. Finally, we can test for influential observations wrt both y and x values. Cook's distance measure test indicate that we don't seem to have any influential observations wrt both x and y values. However, applying studentized residuals statistical test on "mlr_transform_orig" model reveal that we have 28 potential outliers that need to be taken care of. We decided to exclude those 28 outliers and ran a new regression using reduced data set. At last, we have achieved our best model so far. "mlr_transform_orig_bs" model has adjusted R^2 equal to 0.6783, and plots of residuals approximately satisfies the regression assumption even though we have few observations that are possibly outliers wrt to either y or x values.  
