---
title: "Income Regression Analysis: Project report"
output: 
  html_notebook:
    toc: true
    theme: united
author: Chul Ho Choi, Vadym Ovcharenko, Marissa Ashner
---

# Introduction
In this project, we decided to study the relationship between personal variables and income from United States citizens. It was not easy to find a suitable dataset. The data was retrieved from the Minnesota Population Center, a database that allowed for manual selection of the variables to be included in the dataset. After creating the dataset of choice, the data was preprocessed to account for missing values and other inconsistencies. Several multiple linear regression models are applied to preprocessed data set, including a preliminary regression on all variables, a regression on manually selected variables, stepwise regression, and regression on data that was transformed using box-cox transformations. These models are compared based on their $\bar{R}^2$ and the validation of the assumed model assumptions. Additionally, we explore the outliers in the data to make a potentially stronger model without these values. 

# Data preparation

The initially selected variables obtained from the Minnesota Population Center are:

1. YEAR (Census year)
2. DATANUM (Data set number)
3. SERIAL (Household serial number)
4. HHWT (Household weight)
5. GQ (Group quarters status)
6. PERNUM (Person number in sample unit)
7. PERWT (Person weight)
8. NCHILD (Number of own children in the household) 9. SEX (Sex)
10. AGE (Age)
11. MARRNO (Times married)
12. RACE (Race [general version])
13. HCOVANY (Any health insurance coverage)
14. EDUC (Educational attainment [general version]) 15. SCHLTYPE (Public or private school)
16. IND (Industry)
17. WKSWORK2 (Weeks worked last year, intervalled) 18. UHRSWORK (Usual hours worked per week)
19. INCTOT (Total personal income)
20. INCWAGE (Wage and salary income)
21. VETSTAT (Veteran status [general version])
22. VETSTATD (Veteran status [detailed version])
23. PWSTATE2 (Place of work: state)
24. TRANTIME (Travel time to work)

However, there are some complexities of using the original dataset. First of all, the dataset has factor variables of a high level (e.g., up to 10k levels for the industry variable). Moreover, the dataset is presented in the form of a frequency table, with corresponding weight values showing the number of individuals representing each row. Finally, the number of rows is close to 10k, which is too big for the analysis. 

In this section we preprocess the data to account for missing values and other inconsistencies. We aggregate several variables, apply weighting, and sample the dataset to 500 observations. 

## Preprocessing

```{r}
df <- read.csv('dataset.csv')
df$VETSTAT <- as.factor(df$VETSTAT)
df$IND <- as.factor(df$IND)
df$SCHLTYPE_revised <- as.factor(df$SCHLTYPE_revised)
df_raw <- read.csv('dataset_original.csv')
head(df_raw)
```
This is the look of the original dataset. 

```{r}
income$IND_revised <- income$IND
income$IND_revised[income$IND_revised >= 170 & income$IND_revised <= 3990  ] <- "Agriculture, Forestry, Fishing and Hunting, and Mining"
income$IND_revised[income$IND_revised >= 4070 & income$IND_revised <= 4590  ] <- "Wholesale Trade"
income$IND_revised[income$IND_revised >= 4670 & income$IND_revised <= 5790  ] <- "Retail Trade"
income$IND_revised[income$IND_revised >= 6070 & income$IND_revised <= 6390  ] <- "Transportation and Warehousing, and Utilities"
income$IND_revised[income$IND_revised == 570 || income$IND_revised == 580 || income$IND_revised == 590 || income$IND_revised == 670 || income$IND_revised == 680 || income$IND_revised == 690] <- "Transportation and Warehousing, and Utilities"
income$IND_revised[income$IND_revised >= 6470 & income$IND_revised <= 6780  ] <-"Information"
income$IND_revised[income$IND_revised >= 6870 & income$IND_revised <= 7190  ] <- "Finance and Insurance, and Real Estate and Rental and Leasing"
income$IND_revised[income$IND_revised >= 7270 & income$IND_revised <= 7790  ] <- "Professional, Scientific, and Management, and Administrative, and Waste Management Services"
states_revised$IND_revised[states_revised$IND %in% c(570,580,590,670,680,690)] <- "Transportation and Warehousing, and Utilities"
income$IND_revised[income$IND_revised >= 7860 & income$IND_revised <= 8470  ] <- "Educational Services, and Health Care and Social Assistance"
income$IND_revised[income$IND_revised >= 8560 & income$IND_revised <= 8690  ] <- "Arts, Entertainment, and Recreation, and Accommodation and Food Services"
income$IND_revised[income$IND_revised >= 8770 & income$IND_revised <= 9290  ] <- "Other Services, Except Public Administration"
income$IND_revised[income$IND_revised >= 9370 & income$IND_revised <= 9590  ] <- "Public Administration"
income$IND_revised[income$IND_revised >= 9670 & income$IND_revised <= 9920  ] <- "Active Duty Military"
```
Industry variable represents industry codes. We aggregated `IND` variable by aggregating each data point into its appropriate industry category.

```{r}
# aggregating the industry variable
par(mfrow=c(1,2))
plot(df$IND)
plot(df$IND_revised)
```


```{r}
states_revised <- read.csv("dataset.csv")
table(states_revised$PWSTATE2_revised)
table(states_revised$PWSTATE2)
View(states_revised)
states_revised$PWSTATE2_revised <- states_revised$PWSTATE2
states_revised <- states_revised[!(states_revised$PWSTATE2_revised >= 61),]
states_revised <- states_revised[!(states_revised$PWSTATE2_revised == 0),]
states_revised$PWSTATE2_revised[states_revised$PWSTATE2_revised %in% c(53,41,6,30,16,56,32,49,8,4,35,2,15)] <- "WEST"
states_revised$PWSTATE2_revised[states_revised$PWSTATE2_revised %in% c(38,27,46,19,31,20,29,55,26,17,18,39)] <- "MIDWEST"
states_revised$PWSTATE2_revised[states_revised$PWSTATE2_revised %in% c(40,5,48,22,28,1,47,21,54,51,37,45,13,12,10,24,11)] <- "SOUTH"
states_revised$PWSTATE2_revised[states_revised$PWSTATE2_revised %in% c(33,50,23,25,44,9,36,34,42)] <- "NORTHEAST"
```
Transformed `PWSTATE2` by categorizing each data point into four regions of united states.

```{r}
cat_revised$RACE_revised <- cat_revised$RACE
table(cat_revised$RACE)
cat_revised$RACE_revised[cat_revised$RACE_revised == 1] <- "White"
cat_revised$RACE_revised[cat_revised$RACE_revised == 2] <- "Black"
cat_revised$RACE_revised[cat_revised$RACE_revised == 3] <- "American Indian"
cat_revised$RACE_revised[cat_revised$RACE_revised %in% c(4,5,6)] <- "Asian"
cat_revised$RACE_revised[cat_revised$RACE_revised %in% c(7,8,9)] <- "Other"
```
Transformed `RACE` variable by assigning descripitve names rather than just simple numbers.

```{r}
cat_revised$HCOVANY_revised <- cat_revised$HCOVANY
cat_revised$HCOVANY_revised[cat_revised$HCOVANY_revised == 1] <- "No health insurance coverage"
cat_revised$HCOVANY_revised[cat_revised$HCOVANY_revised == 2] <- "With health insurance coverage"
```
Transformed `HCOVANY` variable by assigning descriptive names rather than just simple numbers.

```{r}
cat_revised$WKSWORK2_revised <- cat_revised$WKSWORK2
cat_revised$WKSWORK2_revised[cat_revised$WKSWORK2_revised == 1] <- "1-13 weeks"
cat_revised$WKSWORK2_revised[cat_revised$WKSWORK2_revised == 2] <- "14-26 weeks"
cat_revised$WKSWORK2_revised[cat_revised$WKSWORK2_revised == 3] <- "27-39 weeks"
cat_revised$WKSWORK2_revised[cat_revised$WKSWORK2_revised == 4] <- "40-47 weeks"
cat_revised$WKSWORK2_revised[cat_revised$WKSWORK2_revised == 5] <- "48-49 weeks"
cat_revised$WKSWORK2_revised[cat_revised$WKSWORK2_revised == 6] <- "50-52 weeks"
```
Transformed `WKSWORK2` variable by assigning descripitve names rather than just simple numbers. 

```{r}
cat_revised$VETSTAT_revised <- cat_revised$VETSTAT
cat_revised$VETSTAT_revised[cat_revised$VETSTAT_revised == 1] <- "Not a veteran"
cat_revised$VETSTAT_revised[cat_revised$VETSTAT_revised == 2] <- "Veteran"
```
Transformed `VETSTAT` variable by assigning descripitve names rather than just simple numbers. 

```{r}
# aggregate the education variable
df$EDUC_revised <- as.factor(sapply(df$EDUC, function(x) {
  if (x == 0) return('no school or N/A')
  else if (x < 7) return('1-12 years of pre-college')
  else if (x < 11) return('1-4 years of college')
  else return ('5+ years of college')
}))
par(mfrow=c(1,2))
plot(as.factor(df$EDUC))
plot(df$EDUC_revised)
```
We aggregated the `EDUC` variable into 4 levels that capture high school, college, and advanced college degrees.


```{r}
library(ggplot2)
ggplot(df) +
  geom_point(aes(x = seq(1,nrow(df)), y = df$INCTOT, color=df$EDUC_revised)) +
  theme_bw()
```
Unsuprisingly, there is an obvious correlation between the education level and income.

```{r}
# aggregate the school type variable
df$SCHLTYPE_revised <- as.factor(sapply(df$SCHLTYPE, function(x) {
  if (x == 0) return('N/A')
  else if (x == 1) return('Not Enrolled')
  else if (x == 2) return('Public School')
  else return ('Private School')
}))
par(mfrow=c(1,2))
plot(as.factor(df$SCHLTYPE))
plot(df$SCHLTYPE_revised)
```

We dropped `INCWAGE` variable after realizing it was not very discriptive. We decided to use total income as a response variable.
```{r}
hist(df$INCTOT, breaks = 80, col = "lightgrey", main = "Hostogram of income")
```

```{r}
# drop old variables
dfn <- subset(df, select = -c(MARRNO_revised, RACE, HCOVANY, IND, VETSTAT, PWSTATE2, EDUC, SCHLTYPE))
# rename columns
colnames(dfn) <- c("WEIGHT", "NUM_CHILD", "SEX", "AGE", "NUM_MARR", "WEEKS_WORKED_NUMERIC", "NUM_HOURS_WEEK", "INCOME", "TRAVEL_TIME", "INDUSTRY", "EDUC", "DIVISION", "SCHL_TYPE", "RACE", "HEALTH_INS", "WEEKS_WORKED", "VETERAN")
head(dfn)
```

In the following script, we calculated the weighted (by `WEIGHT`) income for the observations with equal predictor variables.
```{r}
library(data.table)
dt <- as.data.table(dfn)
# group by predictor variables
dt <- dt[,list(INCOME=weighted.mean(INCOME, WEIGHT)), setdiff(names(dt), c("WEIGHT", "INCOME"))]
head(dt)
```
The data is prepared, but the size of the dataset is still around 10,000 observations. 

```{r}
distributions <- function(dt) {
  par(mfrow=c(4,4))
  # show distributions of the variables
  for (i in names(dt)) {
    if (is.numeric(dt[[i]])) {
      hist(dt[[i]], main = i)
    } else {
      plot(dt[[i]], main = i)
    }
  }
}

# visualise distributions
distributions(dt)
```
It seems that taking a sample maintained the general distribution properties of the variables.

```{r}
set.seed(1337)
dts <- dt[sample(nrow(dt), 500), ]
distributions(dts)
```
It can be seen that the general distribution properties are indeed captured in the 500 observation sample subset.

## Documentation
After all preprocessing operations, the final dataset contained 16 variables:

| Variable      | Description |
|---------------|-------------------------------------------------------------|
| NUM_CHILD     | Counts the number of own children (of any age or marital status) residing with each individual. Includes step-children and adopted children as well as biological children.
| SEX           | Reports whether the person was male or female.
| AGE           | Reports the person's age in years as of the last birthday.
| EDUC          | Reports the person's education level.
| NUM_MARR      | Reports the number of times ever-married persons have been married.
| NUM_HOURS_WEEK| Reports the usual number of hours the person earned per week.
| TRAVEL_TIME   | Reports the number of minutes it usually took the person to get home from work.
| INDUSTRY      | Reports the type of industry in which the person performed an occupation. Persons who worked in multiple industries were asked to report from which they earned the most money. Unemployed persons were to report their most recent industry. 
| DIVISION      | Reports the region of the country that the person's primary workplace is located. 
| SCHL_TYPE     | Reports whether the person attended public or private school, if they were enrolled at all. 
| RACE          | Reports the race of the person.
| HEALTH_INS    | Reports whether or not the person has health insurance coverage.
| WEEKS_WORKED  | Reports the number of weeks the person worked last year, intervalled. 
| WEEKS_WORKED_NUMERIC| Reports the number of week the person worked last year.
| VETERAN       | Reports whether or not the person is a veteran.
| INCOME        | Reports the total person's pre-tax personal income from the previous year. This is the response variable in the analysis. 

# Multiple Linear Regression Analysis: 

## "Preliminary" model

In this first model, we decided to run multiple regression on income using all 14 of independent variables in the dataset. But before building the model, we made some final preprocessing steps:

(1) `WEEKS_WORKED_NUMERIC` variable has been removed from the data set
(2) `NUM_MARR` variable is converted from numerical variable to categorical variable
(3) long industry names shortened for prettier model summaries

```{r}
orig_data <- read.csv("dataset_final_sample.csv")
orig_data <- orig_data[, !(colnames(orig_data) %in% c("WEEKS_WORKED_NUMERIC"))]
orig_data$INDUSTRY <- sapply(orig_data$INDUSTRY, function(x) substr(x, 0, 60))
orig_data$NUM_MARR <- as.factor(orig_data$NUM_MARR)
mlr <- lm(orig_data$INCOME ~ ., data = orig_data )
summary(mlr)
```
For this model, $\bar{R}^2$ turnes out to be $0.3092$, which means that the model fails to describe the data accurately. 

### F test and p-value for "preliminary" model

```{r}
qf(0.95, df1 = 38, df2 = 461)
```
Running F-test on the preliminary model reveals that the model is significant at alpha = 0.05 because F-statistic of the model ($6.878$) is greater than $F(0.05) = 1.431182$. Also, p-value is close to being $0$. 

### `VIF` test (multicollinearity test)

```{r}
library(car)
vif(mlr)
mean(vif(mlr)[,1]) # mean of vifs = 1.6133
max(vif(mlr)[,1]) # maximum value of vifs = 2.964278
```
Since mean of vifs = $1.6133$ is not substantially greater than $1$ and maximum value of vifs = $2.964278$ is less than $10$, we can conclude that our "preliminary" model doesn't exhibit multicollinearity.

### Residual analysis of "preliminary" model

```{r}
# Split the plotting panel into a 2 x 2 grid
par(mfrow = c(2, 2), pch = 16, cex = .5)
plot(mlr)
```
Four plots above shows that our model does not satisfy the regression assumptions:

* Specifically, the **Residuals vs. Fitted Values** plot shows an obvious pattern that fans out as the fitted values increase, showing that the variance of the error terms are not constant. 
* The **Normal Probability Plot** shows curvature toward the ends, indicating that the error terms are not normally distributed. 


## "Manual" model

For the second model, we decided to select variables that passed 95% significance test on the "preliminary" model. They are `AGE`, `NUM_HOURS_WEEK`, `EDUC` and `DIVISION`.

```{r}
mlr_t <- lm(orig_data$INCOME ~ AGE + NUM_HOURS_WEEK + EDUC + DIVISION, data =orig_data)
summary(mlr_t)
```
For this model, $\bar{R}^2$ turned out to be $0.3092$, which is the same as the one we got from "preliminary" model. It means that by reducing the dimensionality, we did not lose much information about the predicted variable.

### F test for "manual" model

```{r}
qf(0.95, df1 = 8, df2 = 491)
```
"Manual" model is statistically significant because F-statistic: $28.9$ is greater than $F(alpha = 0.05) = 1.957253$

### Plot "manual" model

```{r}
par(mfrow = c(2, 2), pch = 16, cex = .5)
plot(mlr_t)
```
Four plots above shows that the second model does not satisfy the regression assumptions. Similarly to the first model, The **Residuals vs. Fitted Values** plot, as well as the **Normal Probability Plot** show variations from the expected plots, indicating the error terms are not normally distributed with constant variance. 

## Automatic model building

### Backward elimination

```{r}
set.seed(1337)
step(mlr, direction = "backward", trace = F)
```
Running backward elimination method reveals that we should include `SEX`, `AGE`, `NUM_HOURS_WEEK`, `EDUC`, `DIVISION` as our independent variables. It should be noted that these variables are the same as the significant variables chosen for the manual model above, with the addition of the `SEX` variable here. 

### Stepwise regression

```{r}
step(mlr, direction = "both", trace = F)
```

Running stepwise regression method reveals that we should include `SEX`, `AGE`, `NUM_HOURS_WEEK`, `EDUC`, `DIVISION` as our independent variables. This is the same conclusion drawn from the backward elimination. 

## Multiple Linear Regression: "Automatic" model

The automatic model building procedure suggested that we should use `SEX`, `AGE`, `NUM_HOURS_WEEK`, `EDUC`, `DIVISION` as our independent variables. 
```{r}
mlr_bs <- lm(orig_data$INCOME ~ SEX + AGE + NUM_HOURS_WEEK + EDUC + DIVISION, data = orig_data)
summary(mlr_bs)
```
Adjusted $\bar{R}^2$ turned out to be $0.3108$, which is slightly better than the first models. 

### Residual analysis of "automatic" model

#### Residual plots

```{r}
par(mfrow = c(2, 2), pch = 16, cex = .5)
plot(mlr_bs)
```
Although the $\bar{R}^2$ was slightly improved compared to the prior models, the residual plots still shows that our model doesn't satisfy the regression assumptions.  

#### Histogram plot of "automatic" model's residuals.

```{r}
hist(mlr_bs$residuals, col = "lightgray")
```

Plot of residuals confirms that they are definitely not distributed normally, but in fact are positively skewed. We can try to apply some transformations to normalize the residuals.

### Finding Box-Cox transformation's value of lambda

```{r}
library(MASS)
b_bs <- boxcox(mlr, lambda = seq(0,0.2,.1))
boxcox_values_bs <- cbind(b_bs$x,b_bs$y)
value_bs <- boxcox_values_bs[order(-b_bs$y),][1,1]
value_bs
```
Running Box-Cox test on the original data reveals that we should use $\lambda = 0.01414141$ for our transformation on our dependent variable which is income. It can be seen from the above plot that the maximum likelihood estimation has the highest peak at $\lambda = 0.01414141$.

## Box-Cox transformed model

We created a new model using transformed predicted variable. Re-running backward elimination again (steps omitted) showed that we should include `AGE`, `NUM_HOURS_WEEK`, `TRAVEL_TIME`,`INDUSTRY`, `EDUC`, `DIVISION`, `SCHL_TYPE`, `HEALTH_INS`, `WEEKS_WORKED`. It is notable that these variables are different than the ones picked out from the non-transformed data: `SEX`, `AGE`, `NUM_HOURS_WEEK`, `EDUC`, `DIVISION`.

```{r}
orig_data$INCOME_TRANSFORMED <- orig_data$INCOME^value_bs
mlr_transform_bs <- lm(formula = orig_data$INCOME_TRANSFORMED ~ AGE + NUM_HOURS_WEEK + 
    TRAVEL_TIME + INDUSTRY + EDUC + DIVISION + SCHL_TYPE + HEALTH_INS + WEEKS_WORKED, data = orig_data)
summary(mlr_transform_bs)
```

The transformed model has $\bar{R}^2$ equal to $0.6029$, which is a big improvement from all previous models. 

## Residual analysis of the transformed model

### Residual plots

```{r}
par(mfrow = c(2, 2), pch = 16, cex = .5)
plot(mlr_transform_bs)
```
The transformed model still seems to not quite satisfy the regression assumptions. There are, however, improvements in the **Normal Probability Plot** and **Residuals vs. Fitted Values** plot, indicating that this model more accurately follows the assumptions of constant variance and normality than the previous models discussed:

(1) The Normal Probability Plot appears fairly linear, indicating the normality assumption may be satisfied. 
(2) The Residials vs. Fitted Values plot doesn't appear to have any obvious patterns, with only slight fanning out for larger fitted values (suspected outliers). There is no clear indication that the constant variance assumption may be invalidated. 

### Histogram of the transformed model's residuals

```{r}
hist(mlr_transform_bs$residuals, col = "lightgray")
```
Histogram plot above can be said appproximately normal, further indicating that the normality assumption may hold for the transformed model. However, the Anderson-Darling (AD) Statistic test can be run to further determine whether or not the normality condition is met. 

### Anderson-Darling statistic test
```{r}
library(nortest)
ad.test(mlr_transform_bs$residuals)
```
Applyting AD test on our box-cox transfomed and stepwise regression applied model reveal that normality assumption is not satisfied because we can reject the null hypothesis of AD test at $\alpha = 0.05$. 

### Cook's Distance Measure plot for the transformed model

```{r}
library(car)
plot(mlr_transform_bs, pch = 18, col ="red", which = c(4)) 
cook <- cooks.distance(mlr_transform_bs)
abline(h = 4/nrow(orig_data), col="blue")  # add cutoff line
```
According to the plot of the Cook's distances, there might be several outlying values. 

```{r}
k <- length(coef(mlr_transform_bs)) - 1
f_50 <- qf(0.50, df1 = k+1, df2 = mlr_transform_bs$df)
f_80 <- qf(0.20, df1 = k+1, df2 = mlr_transform_bs$df)
any(cook > f_50, na.rm = T)
```
There seems to be no influential observations according to Cook's distance test described in the textbook. None of the observations have cook's distance values greater than $F_{[0.5]}$, but every observation has a Cook's distance value less than $F_{[0.8]}$. Some sources suggest different cut-off values to use for spotting highly influential points. 

```{r}
omit_cook <- which(cook > 4/nrow(orig_data))
length(omit_cook)
```
Using the cut-off value $D_i>4/n$, where $n$ is the number of observations (shown as a horisontal line on the plot above), we detected 39 outlying values.

### Studentized residuals of the transormed model

```{r}
stud_residual <- rstandard(mlr_transform_bs) # studentized residuals
omit_stud <- which(stud_residual < -2 | stud_residual > 2 | is.na(stud_residual)) #studentized residuals greater than absolute value of 2 and na values
length(omit_stud) # 28 potential outliers wrt y values
```
After studentizing residuals for transformed model, we selected 29 potential outliers with respect to $y$ values. 

###  Leverage values for the transformed model

```{r}
lev <- hatvalues(mlr_transform_bs)
cutoff_lev <- 2 * (k +1) / n
plot(lev, pch = 16, cex = .5)
abline(h = cutoff_lev, col="red")  # add cutoff line
omit_lev <- which(lev > cutoff_lev)
length(omit_lev) 
```
There are 25 observations that are associated with large leverage values.

### Dropping potential outliers from the transformed model

Studying the nature of outliers is beyond the scope of this analysis, so we decided to see how the model will improve if we exclude the outliers with respect to $y$ them from the model. We didn't exclude outliers wrt to $x$ because it showed to decrease the utility of the model. 

```{r}
orig_data_stud_omit <- orig_data[-c(omit_stud, omit_cook),]
mlr_transform_orig_bs_omit <- lm(INCOME_TRANSFORMED ~ AGE + NUM_HOURS_WEEK + 
    TRAVEL_TIME + INDUSTRY + EDUC + DIVISION + SCHL_TYPE + HEALTH_INS + 
    WEEKS_WORKED, data = orig_data_stud_omit)
summary(mlr_transform_orig_bs_omit)
nrow(orig_data) - nrow(orig_data_stud_omit) # number of excluded observations
```
After dropping 47 observations that were classified as potential outliers model has $\bar{R}^2$ equal to $0.6957$. This is notably increased from the $0.6029$ value before the outliers were dropped. 

### Anderson-Darling statistic test
```{r}
library(nortest)
ad.test(mlr_transform_orig_bs_omit$residuals)
```
Applyting AD test on our final best model reveal that normality assumption is satisfied because we fail to reject the null hypothesis of AD test at $\alpha = 0.05$.

### Residual Plots
```{r}
par(mfrow = c(2, 2), pch = 16, cex = .5)
plot(mlr_transform_orig_bs_omit)
```
4 plots above seem to indicate that the model without suspected outliers satisfies the regresion assumptions. The **Normal Probability Plot** seems linear, and the **Residuals vs. Fitted Value** Plot shows only minor signs of violation of the constant variance assumption (slight fanning out). 

## Interpreting the model
Our final model demonstrates satisfactory adherence to the regression assumptions and has a relatively high adjusted R-squared score of 0.6957. It means that we can draw somewhat reasoned conclusions from its interpretation. 

Unsurprisingly, some of the most significant predictors are `NUM_HOURS_WEEK` and `WEEKS_WORKED` because they are usually highly correlated with a person's income. More interesting relationships are demonstrated by the variables corresponding to Northeastern division of the United States, health insurance coverage, 5+ years of college, public school type, etc. To understand the relationships of these variables to the person's income their estimated coefficients must be interpreted. Interpreting the coefficients of a model with transformed predicted variable turns out to be a difficult task. Since the value of $\lambda = 0.01414141$ in our transformation was close to zero, we decided to follow the procedure described [here](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-a-regression-model-when-some-variables-are-log-transformed/) assuming that the transformation was logarithmic. Thus, we can interpret estimated coefficients to be an approximate percentage change in the person's income. For example, the estimated coefficient for a health insurance variable (`HEALTH_INS`) is $5.516e-03$. $100(e^{5.516e-03} - 1) = 0.55$ means that the expected difference in yearly income is 0.55% for an individual with health insurance compared to an individual without one.

Some of the most interesting regression analysis results can be summed up as follows:

* Individuals with 5+ years of college have 1.53% higher income
* Individuals working in the Northeastern division have 0.6% higher income than those working in Midwest
* Each additional commute hour increases income by 0.4%
* Every ten years, individual's income increases by 0.2%

Since `SEX` variable was not included in the final model, we conclude that gender does not have a significant relationship to the individual's income.

# Conclusion
Our preliminary model has adjusted $\bar{R}^2 = 0.3092$ and F-test indicates that the overall model is significant at $\alpha=0.05$. However, plotting residuals reveal that the preliminary model doesn't satisfy the regression assumptions. The manual model doesn't do any better than the preliminary model in terms of adjusted $\bar{R}^2$. The `vif` test reveals that the preliminary model doesn't seem to exhibit any serious multicollinearity among the independent variables. The model was further narrowed down by runnning both backward elimination and stepwise regression on preliminary model. Both methods leave us with `SEX`, `AGE`, `NUM_HOURS_WEEK`, `EDUC`, and `DIVISION` variables to run regression on income variable. This automatic model does slightly better than the preliminary model in terms of adjusted $\bar{R}^2 = 0.3108$, but the plot of residuals shows that "automatic" model still doesn't satisfy the regression assumptions. 

In order to meet the regression assumptions, we had to transform income, the dependent variable, using the box-cox transformation method. If we box-cox transform our income variable, we get a preliminary transformed model, which has adjusted $\bar{R}^2 = 0.6029$. Then, we sequentially apply stepwise regression or backward elimination method on this model. Both methods give us two identical models using same independent variables. Finally, we can test for influential observations wrt both y and x values. We decided to exclude outliers wrt y and ran a new regression using reduced data set. At last, we have achieved our best model so far. The preliminary transformed model has adjusted $\bar{R}^2 = 0.6957$ after stepwise regression, and plots of residuals approximately satisfies the regression assumption even though we have few observations that are possibly outliers wrt to x values.  

Lastly, we give an interpretation of the final model. We faced a [problem](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-a-regression-model-when-some-variables-are-log-transformed/) of coefficient interpretation of a transformed model. Here, to interpret the effect of a predictor variable to the outcome variable, we have to:

(a) Assume the transformation to be a natural logarithm transformation (since the value of $\lambda$ in our transformation was close to zero);
(b) Approximate the inverse of the transformation to be exponentiation $e^β$ (since exponentiation is the inverse of logarithm function).

As a result, exponentiated coefficients approximately correspond to changes in the ratio of the expected geometric means of the original outcome variable. Consequently, we study the relationships of the most influential variables and present a summary of the most important conclusions.

# Data Reference 
Steven Ruggles, Katie Genadek, Ronald Goeken, Josiah Grover, and Matthew Sobek. Integrated Public UseMicrodata Series: Version 7.0 [dataset]. Minneapolis, MN: University of Minnesota, 2017.https://doi.org/10.18128/D010.V7.0
